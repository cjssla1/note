자연어 처리
Bag of Words : 단어들의 빈도를 표로 표현한 것
장점: 문장의 유사도를 알 수 있다. (각각의 빈도를 나타낸 벡터를 내적한 값)
단점: 단어들의 순서를 무시한다. Sparsity, 단어의 개수가 커짐에 따라 입력이 커져 힘들다. 빈도가 많을수록 더 큰 힘을 얻는다. 처음 본 단어를 처리 못한다.

N-gram: n개의 토큰(문자, 형태소, 단어)으로 구성
N이 1 일 때 유니그램(unigram), 2일 때 바이그램(bigram), 3일 때 트라이그램(trigram)
장점 : 단어의 순서 조금 유지, 다음 단어 예측 가능, 오타 발견 가능

TF-IDF( Term Frequency * Inverse Document Frequency ) : 각 단어와 문서의 연관성을 수치도 나타낸 값
TF : 문서에서 단어가 출현한 빈도, TF score = 단어 / 전체 단어, 빈도가 높을수록 문장에 중요할 것이다.라는 가정 그러나 관사나 문법이 더 많이 출현하기에 TF만으론 부족하다.
IDF: Log( 전체 문장 개수 / 출현한 문장 개수 + 1(스무딩)) 문장에 관계없이 자주 나오는 단어에 불이익을 준다.

자연어처리의 유사도 판단 방법: 코사인 유사도, 유클리드 거리
유클리드 거리 : 피타고라스 정리를 이용. 백터 크기 문제 발생
코사인 유사도 : A'B내적 / A크기*B크기

문서 유사도 판단:
 단순 단어의 back of words 또는 TF-IDF의 back of words을 사용하여 코사인 유사도를 통하여 판단
 TF-IDF의 장점: 계산이 쉽다. 중요한 단어를 살려줌.
 TF-IDF의 단점: 동음이의어에 취약, 문맥적 의미가 없음, 토픽을 알기 어려움

잠재 의미 분석(Latent Semantic Analayze)
 문장속 단어의 빈도를 행렬로 표시한 후 특이값 분해(SVD)을 수행하여 벡터들의 차원을 축소하여 표현한다.
 SVD 사용하지 않을 시 입력이 매우 커 입력으로 사용하기 힘들다.
 
Word2Vec
 딥러닝에서 입력을 위해 인코딩이 필요함, 가장 쉬운 방법은 원핫인코딩이나 Sparsity문제, 유사한 단어 구별(동음이의어 등), 유사도 계산 못함 
 그리하여 사용한 방법이 바로 Word2Vec 워드 임베딩이다.(단어를 원핫인코딩대신 특정 수의 벡터로 표현한다. 특정 수는 원핫인코딩보다 훨씬 낮다)
 
WMD
 word2vec을 이용하여 문서간 유사도 비교, 느림

RNN
 
